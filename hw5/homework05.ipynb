{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework05.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "gIp6dzlwIbw0",
        "j_-dO56ledsy",
        "5AMarkOTyVt3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "_4pJohDkIbwu",
        "tags": [
          "T"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "> **Student Names and IDs**:\n",
        ">\n",
        "> - Student 1, ID1 (Replace this item with your first and last name and student ID number.\n",
        "Add more items like this as needed, including the `> -` characters at the beginning,\n",
        "which generate the indent and the bullet.)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "93F4Xn6mIbww",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aymIv7MFIbwz",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Homework Submission Workflow\n",
        "\n",
        "When you submit your work, follow the instructions on the [submission workflow page](https://www.cs.duke.edu/courses/fall18/compsci371d/homework/workflow.html) for full credit, but see the changes mentioned below.\n",
        "\n",
        "**Important: Failure to do any of the following will result in lost points:**\n",
        "\n",
        "- Submit **one** PDF file and **one** notebook per group\n",
        "\n",
        "- Enter **all the group members** through the Gradescope GUI when you submit your PDF files. It is **not** enough to list group members in your documents\n",
        "\n",
        "- Match each **answer** (not question!) with the appropriate page in Gradescope\n",
        "\n",
        "- Avoid large blank spaces in your PDF file\n",
        "\n",
        "**Important changes to homework preparation workflow:** _This assignment is different from the others in that you are required to run it on the Google Colaboratory, a cloud service that Google makes available for reseach in machine learning. This is necessary because some of the problems require you to train a deep network on hardware that is faster than what is typically available on a standard laptop or desktop, including a high-end GPU. Even if you do have a high-end GPU, please run your notebook on the Colaboratory, so we can grade your work consistently._\n",
        "\n",
        "_**To work on this assignment, go to the [Colaboratory](https://colab.research.google.com) and upload the template notebook for this assignment through the `File` menu at the top of the Colaboratory page. Then work on the assignment, making sure to pay attention to instructions in Part 4 where you are asked to change the runtime type.**_\n",
        "\n",
        "_**When you are done, download the notebook (after making sure that all the outputs from running the code show up properly), and proceed as usual to turn that notebook into a PDF file for submission.**_\n",
        "\n",
        "#### Programming Notes\n",
        "\n",
        "+ The Colaboratory is a cloud service. If a notebook sits idle for a long time, it automatically disconnects from its execution kernel, and you need to rerun all the cells.\n",
        "+ Some of the cells in the Part on neural networks are to be run with different runtime types, as explained later. Because of this, you will not be able to just restart the notebook and run all its cells with a single command. Instead, you need to run the cells one at a time, changing runtime type as instructed. Make sure you do this once you are done with the assignment, making sure that the output from your code matches the text where you describe that output.\n",
        "+ Depending on circumstances, changing the runtime type may erase some or all of the notebook state. This will require you to rerun the cells that generate state.\n",
        "+ Training depends on random initialization of the network parameters. Because of this, your results may vary relative to the sample solution, even if your code is no different. Results may also vary from run to run."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gIp6dzlwIbw0",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1: Exam-Style Questions, Set 1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6o90sGDiK4aF",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "The small neural net in the figure below uses the ReLU as the nonlinearity at the output of each neuron. The values specified in the hollow circles are biases, and the values along the edges are gains. Weigths number 1, 2, 3 refer to the first neuron, 4, 5,6 to the second, 7, 8, 9 to the third."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CakDYmy0Mk13",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "![a simple neural network](https://www2.cs.duke.edu/courses/spring19/compsci527/homework/5/netSimple.png)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tdrtWLQ5JZ7q",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 1.1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CacoODWaJdmL",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Are all the layers in the net above fully connected?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WJ9w1-GpN1FM",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4o_0Co16OJ-v",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 1.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WmVrDJydN7OP",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What is the output $y$ from the net above when the input is as follows?\n",
        "\n",
        "$$\n",
        "x_1 = 0 \\;\\;\\; \\text{and}\\;\\;\\; x_2 = 3\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "G-LZGjFMORa8",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rP4qW1wUQapl",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 1.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2WZXG8wNQeJ5",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What is the gradient $\\mathbf{g}$ of the output $y$ of the network above with respect to the weight vector\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [w_1,\\ w_2,\\ w_3,\\ w_4,\\ w_5,\\ w_6,\\ w_7,\\ w_8,\\ w_9]^T\n",
        "$$\n",
        "\n",
        "when the input has the values given in the previous problem? Just give the result if you are confident of your answer."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5fVXbN93RDq7",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "j_-dO56ledsy",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part 2: Exam-Style Questions, Set 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CWV3624YZjhu",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Let $\\mathbf{p} = f(\\mathbf{x})$ be the output of the network's soft-max layer of some neural network classifier with $K$ layers when the network's input is $\\mathbf{x}$. The classifier's output is then\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\arg\\max \\mathbf{p}\\;.\n",
        "$$\n",
        "\n",
        "If $y_n$ is the true label corresponding to training input $\\mathbf{x}_n$, the loss is $\\ell_n = \\ell(y_n, f(\\mathbf{x}_n))$ for some appropriate loss function $\\ell(y, \\mathbf{p})$.\n",
        "\n",
        "We saw in class that if $\\mathbf{x}^{(k)}$ is the output from layer $k$ and $\\mathbf{w}^{(k)}$ is a vector with all the parameters in layer $k$, then back-propagation computes the partial derivatives by the following recursion,  where $\\mathbf{x}^{(0)} = \\mathbf{x}$ is the input to the network and $\\mathbf{x}^{(K)} = \\mathbf{p}$:\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}\n",
        "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 1 \\\\\n",
        "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}}\n",
        "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 2\\\\\n",
        "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(K)}} &=& \\frac{\\partial \\ell}{\\partial \\mathbf{p}}\n",
        "\\end{eqnarray*}\n",
        "\n",
        "The derivatives above are computed for the $n$-th training sample $(\\mathbf{x}_n, y_n)$ and for the values of $\\mathbf{w}^{(k)}$ that are current at any given point during training."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cNK7pnfDesaH",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vY-vJZ_Peu9c",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Suppose that the network has only fully-connected layers (with ReLU nonlinearities) before the soft-max. Refer in detail to the equations given above to explain clearly why training would not work if the parameter vector $\\mathbf{w} = [\\mathbf{w}^{(1)},\\ldots, \\mathbf{w}^{(K)}]^T$ is initialized with zeros for training."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "I5CVu5NXhUpK",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2Vx-msEHjfdn",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NJnaTzLfjiGt",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "A neural net classifier with only fully-connected layers (with ReLU nonlinearities) and a soft-max layer at its output has parameter vector $\\mathbf{w}$, and the network implements the function $f(\\mathbf{x}, \\mathbf{w})$ for any network input $\\mathbf{x}$. Is $\\mathbf{w} = \\mathbf{0}$ a stationary point for the function $\\phi(\\mathbf{w}) = f(\\mathbf{x}, \\mathbf{w})$ when $\\mathbf{x}$ is fixed? Justify your answer."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OxjAkNkzkDHo",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Rla5OsKGk1zi",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lE3Uoh6Hk5jp",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Stochastic gradient descent with momentum is used to train a certain neural network with $m$ parameters. Just before iteration $t$ of training is performed, the parameter vector has value $\\mathbf{w}_t$, and the velocity (or step) is $\\mathbf{v}_t = \\mathbf{a}$, where $\\mathbf{a}$ is some nonzero vector in $\\mathbb{R}^m$ (refer to the class notes for notation). The momentum coefficient is kept constant at $\\mu = 0.9$ throughout training. If the risk function has a saddle point at $\\mathbf{w}_t$, what is the step $\\mathbf{v}_{t+1}$ at iteration $t$?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qTU6DAPRn1hN",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QwMq7SK4nFwU",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.4"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iEaE7twinHyl",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "A friend of yours argues that in the situation described in the previous problem, the steps after iteration $t$ decay exponentially. Her argument is based on the fact that the risk is at a saddle point at $\\mathbf{w}_t$, and the momentum coefficient is constant, so that $\\mathbf{v}_{t+\\tau} = \\mu^{\\tau}\\mathbf{a}$, an exponential decay. Explain why your friend's argument is wrong."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mA05ns5Unt_6",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lfRyRIt3qCoz",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2.5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RWfz9zLXqFAV",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "In the situation described in Problem 2.3, will the training algorithm always eventually converge back towards $\\mathbf{w}_t$? Explain your answer briefly and clearly."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zv-spO4mqZcT",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5AMarkOTyVt3",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3: Exam-Style Questions, Set 3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "74EwMXGkyYgz",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "The following problems take you through the computation of the set of all least-squares solutions to the following linear system:\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "3x + 4y &=& 2\\\\\n",
        "3x + 4y &=& 3\n",
        "\\end{eqnarray*}\n",
        "\n",
        "and the solutions to a related optimization problem.\n",
        "All the answers to the questions in this problem are numerical and exact. They refer only to the data given in the problem, and no more general answers are required. You may leave your answers in the form of fractions, with expressions like the following:\n",
        "\n",
        "$$\n",
        "\\frac{\\sqrt{3}}{2} \\left[\\begin{array}{c} 2\\\\-5\\end{array}\\right]\\;,\n",
        "$$\n",
        "\n",
        "but please simplify as much as possible.\n",
        "\n",
        "_As usual, it is easiest to answer these questions using software (and perhaps guess the exact values from the approximate ones output by your code). However, this would rob you of the opportunity to understand this material and to practice for the exam. **In any event, no answers will be accepted to problems in this part that embed software in your submission.**_\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ac8j8SGdzDr9",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.1"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9R-zasYizH16",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What are $A$ and $\\mathbf{b}$ if we write the system in this problem in the following form?\n",
        "$$\n",
        "A \\mathbf{x} = \\mathbf{b}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "v6jJC2xuzKS5",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eJoEvX1bzPDh",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zA4CavTxzReO",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "What is the rank of $A$?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aplEQX65zUHW",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KOrmLSUVzWM6",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MUH5dslBzapt",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Give a _unit_ column vector $\\mathbf{r}$ that spans the row space of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "p7vpRTgKzfEi",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DbW0YgUWzjQc",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.4"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "q6kAPv09zlWk",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Give a _unit_ column vector $\\mathbf{n}$ that spans the null space of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "23yZJ6GCzo2K",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nOm2a0USzwU0",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cMNnKIARzyfL",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Write the matrix $V$ in the SVD $A = U\\Sigma V^T$ of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "c7U8WqRUz0ct",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zk41d7sBz5GS",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.6"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZK3cYMIkz7Jc",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Compute the matrices $U$ and $\\Sigma$ in the SVD of $A$. [Hint: compute $U\\Sigma$ first.]"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T3ZNlDaUz9gQ",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xHCt-FvH0FDT",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.7"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EhxCKF6U0HwH",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Compute the pseudo-inverse $A^{\\dagger}$ of $A$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "63rXWdST0KZN",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hllY12AA0PKi",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.8"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MnjLAWvt0Q06",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Find the minimum-norm least-squares solution $\\mathbf{x}^*$ of the system $A\\mathbf{x} = \\mathbf{b}$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2RgECzBj0TVw",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MDz6wROp0ZIr",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.9"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0P5mlHPz0bTA",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Give an expression for the set $S$ of all least-squares solutions of the system $A\\mathbf{x} = \\mathbf{b}$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "K2ZKErfi0dwK",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8vQg6So00ij9",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3.10"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ghlx7z5s0lkG",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Find all the solutions to\n",
        "\n",
        "$$\n",
        " \\hat{\\mathbf{x}} = \\arg\\min_{\\|\\mathbf{x}\\| = 1} \\|A\\mathbf{x}\\|\\;.\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wXELtk700oFd",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8GZdQ7IsoUVR",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "## Part 4: Neural Networks"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4gQIvWQAzhzm",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "The code in this part is somewhat modified from the [Keras documentation](https://keras.io/examples/cifar10_cnn/). It downloads the CIFAR-10 dataset, a set of 60000 labeled images grouped in 10 categories, which it splits into training, validation, and test sets. It then defines a function `network` that returns a simple convolutional neural network (the `model`), and a function `train` that trains the model for a single epoch by default, checking performance on the validation set. The function `train` also saves the trained model in a file in the cloud and evaluates the model on the test data. Finally, it returns a history of training and validation accuracies achieved after each epoch of training. The function `train` uses SGD as the default optimizer.\n",
        "\n",
        "_**Important:**_ Make sure you select Python 3 through the `Runtime->Change runtime type` menu at the top of the notebook. Also set the hardware acceleration to `None` in that same menu. We will turn on GPU acceleration later on. TPU acceleration is not always available, so we won't use it."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JM7Sh_kizkO-",
        "outputId": "9f7bf39e-712b-4fc3-a2b6-48cdd5687663",
        "tags": [
          "HST"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# The data, split between train, validation, and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "(x_train, x_validate, y_train, y_validate) = train_test_split(x_train, y_train,\n",
        "                                                             test_size=0.2)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_validate.shape[0], 'validation samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_validate = x_validate.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_validate /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 86s 1us/step\n",
            "x_train shape: (40000, 32, 32, 3)\n",
            "40000 training samples\n",
            "10000 validation samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FBMRAFNN5k6S",
        "tags": [
          "HST"
        ],
        "outputId": "27505960-3b3d-4f02-b88c-0ea240059d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "activation_function = 'relu'\n",
        "\n",
        "def network(activation_function='relu'):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                  input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = network()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "35sd9xJr55eM",
        "tags": [
          "HST"
        ],
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from math import ceil\n",
        "\n",
        "def train(model, epochs=1,\n",
        "          opt = keras.optimizers.SGD(lr=0.01, momentum=0.7, decay=0.001),\n",
        "          verbose=2):\n",
        "\n",
        "  batch_size = 32\n",
        "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "  model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "  # Configure the model for training\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      validation_data=(x_validate, y_validate),\n",
        "                      shuffle=True,\n",
        "                      verbose=verbose)\n",
        "\n",
        "  # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "\n",
        "  # Score trained model.\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "  return [history.epoch, history.history['acc'], history.history['val_acc']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JMElXg2ETjAE",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.1\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oDnPx1gdT5Jm",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Using Stochastic Gradient Descent (SGD) with the default parameters in `train`, train the model for one epoch _with no hardware acceleration_.\n",
        "\n",
        "Show your call to `train` and the outputs it generates. Is validation accuracy a reasonably good estimate of test accuracy?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "73TQApgKiGY_",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "#### Programming Notes\n",
        "\n",
        "+ Hardware acceleration is turned off through the `Runtime->Change runtime type` menu at the top of the notebook, and selecting `None` for hardware acceleration.\n",
        "\n",
        "+ Depending on circumstances, after you change the runtime type, some or all of the notebook state may be lost. This will require you to rerun some of the cells above.\n",
        "\n",
        "+ Tensorflow may generate warning messages that depend on how the Colaboratory interface is implemented. These messages are typically harmless."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5sYqNFyUVxdy",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "iHSJs4KdTfgp",
        "colab_type": "code",
        "outputId": "fc31938e-8784-4e67-ba3f-4ae3c9a26a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "cell_type": "code",
      "source": [
        "train(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            " - 218s - loss: 1.9782 - acc: 0.2718 - val_loss: 1.6760 - val_acc: 0.3844\n",
            "10000/10000 [==============================] - 16s 2ms/step\n",
            "Test loss: 1.6521208040237427\n",
            "Test accuracy: 0.396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0], [0.2718], [0.3844]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "2KcyGxO6dXko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yes, validation accuracy seems to be a reasonably good estimate of test accuracy. Validation accuracy is 38.44%. Test accuracy is 39.60%"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NoGPF3W7WU_X",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yCDoSETDWXCJ",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Repeat the previous experiment _after turning on GPU acceleration_ from the `Runtime->Change runtime type` menu.\n",
        "\n",
        "Are the accuracy values the same as before? Explain why or why not. What is the approximate ratio of running times of CPU (no acceleration) versus GPU training?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JRnVqg5gXbvx",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "R8EALgLweJe-",
        "colab_type": "code",
        "outputId": "51b9ff17-bdb0-496d-bd7a-8bbf627a8f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "cell_type": "code",
      "source": [
        "train(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            " - 21s - loss: 2.0015 - acc: 0.2606 - val_loss: 1.7166 - val_acc: 0.3831\n",
            "10000/10000 [==============================] - 2s 163us/step\n",
            "Test loss: 1.711593028640747\n",
            "Test accuracy: 0.3812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0], [0.260625], [0.3831]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "2FITR73Of0eu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Accuracy values are about the same as before. I repeated some experiments. Sometimes, GPU training resulted in higher test and validation accuracy. Sometimes, CPU training did.\n",
        "\n",
        "For this experiment, CPU training took 218s while GPU training took 21s. The approximate ratio of runtime of CPU versus GPU training is 10. That is, the runtime of CPU training was approximately ten times that of GPU training. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UL293H0_YerN",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hw67mTPzYg-4",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "We keep GPU acceleration turned on from now on.\n",
        "\n",
        "Repeat the experiment above with the ADAM optimizer with the default parameters. This optimizer selects the descent step size adaptively. The ADAM optimizer is invoked by using parameter `opt = keras.optimizers.Adam()` in `train`.\n",
        "\n",
        "Compare accuracies and running times with those achieved in the previous experiment."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PRFT6uhiZdIj",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "Ohg3olyjiqsY",
        "colab_type": "code",
        "outputId": "c82e9ed2-13ff-4230-eeeb-f6a547f3a2cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "train(model, opt = keras.optimizers.Adam())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            " - 18s - loss: 1.5904 - acc: 0.4202 - val_loss: 1.2326 - val_acc: 0.5586\n",
            "10000/10000 [==============================] - 2s 165us/step\n",
            "Test loss: 1.2449142942428588\n",
            "Test accuracy: 0.5531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0], [0.420175], [0.5586]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "5Xf9l12zjRea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test accuracy increased! From less than 40% in 4.1 and 4.2 to 55.31% in 4.3. Validation accuracy increased by a similar margin, to 55.86%.\n",
        "\n",
        "Runtime decreased again, to 18s. It was 21s in 4.2 and 218s in 4.1."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KEp7_h36bC7R",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.4"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UxdkfA9ibFUM",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "We use the ADAM optimizer with default parameters from now on.\n",
        "\n",
        "Repeat the previous experiment with 30 epochs of training instead of 1 (`epochs=30`). This time, store the value returned by `train`, as you will need it for plotting.\n",
        "\n",
        "When done, plot both training accuracy and validation accuracy as functions of epoch number on the same diagram. Label the axes and add a legend to specify which plot is which.\n",
        "\n",
        "Do you think that the classifier would perform much better if you were to train longer? Explain briefly."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Pw9glyaNvwQ6",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "#### Programming Notes\n",
        "\n",
        "+ Look at the definition of `train` to figure out what the output from that function contains.\n",
        "+ Set the value of the `verbose` parameter in the call to `train` to 0 to suppress output, which would be too long to include in your PDF file. You can estimate from your previous experiments how long the code will take to run. Alternatively, set `verbose` to 2 in early test runs, but then set it to 0 in your final run."
      ]
    },
    {
      "metadata": {
        "tags": [
          "ST"
        ],
        "id": "s9FRK1OuRoAf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "EkPUcWbel5wJ",
        "colab_type": "code",
        "outputId": "e0ee0bbf-f2b1-41c8-9ce8-02bb4c0a54a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "history = train(model, epochs=30, opt=keras.optimizers.Adam(), verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 162us/step\n",
            "Test loss: 0.7111205273628235\n",
            "Test accuracy: 0.7778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5oPChB2OqSCh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hekrMEyRqWPw",
        "colab_type": "code",
        "outputId": "c918a7a2-7517-4614-a1ff-9cdcaed0ce63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(history[0], history[1], label='Training Accuracy')\n",
        "plt.scatter(history[0], history[2], label='Validation Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\", rotation=0, labelpad=30)\n",
        "plt.legend(loc=7)\n",
        "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFcCAYAAACUbPdEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XtgU/X9//Fnmt4oDdBAwl3FIgLV\nTupl8q0TxVaZzLmhk3oBFRSdMJ2KivVSp7aAqF9vu4Di3ACxjnX76ZcpgqDzwkUFQeoFwYmgSJO2\ntA1t0zbN74+OSEla2vQ0aZrX46+enJ5zPufNoeedz9Xk9Xq9iIiIiBggJtwFEBERke5DiYWIiIgY\nRomFiIiIGEaJhYiIiBhGiYWIiIgYRomFiIiIGCZqE4sdO3aQlZXF0qVL/fa9//77XHrppUyePJnf\n//73YSidiIhIZIrKxKK6upqHHnqIsWPHBtz/8MMP8/TTT7N8+XLee+89du7cGeISioiIRKaoTCzi\n4+N59tlnsdvtfvv27NlD7969GThwIDExMYwbN47169eHoZQiIiKRJyoTi9jYWBITEwPuczgcWK1W\n37bVasXhcISqaCIiIhEtKhMLozU0eMJdBBERkS4hNtwF6GrsdjtOp9O3vX///oBNJocrL6/u1DLZ\nbBYcjqpOvUYkUTz8KSb+FBN/iok/xaS5tsbDZrO0uE81FkcYMmQILpeLvXv30tDQwLp168jMzAx3\nsURERCJCVNZYbN++nfnz5/Ptt98SGxvLqlWrGD9+PEOGDCE7O5sHHniA22+/HYALL7yQYcOGhbnE\nIiIikSEqE4uTTjqJJUuWtLj/9NNPp7CwMIQlEhER6R7UFCIiIiKGUWIhIiIihlFiISIiIoZRYiEi\nIiKGUWIhIiIihlFiISIiIoZRYiEiIiKGUWIhIiIihlFiISIiIoZRYiEiIiKGUWIhIiIihlFiISIi\nIoZRYiEiIiKGUWIhIiIihlFiISIiIoZRYiEiIiKGUWIhIiIihlFiISIiIoZRYiEiIiKGUWIhIiIi\nhlFiISIiIoaJysSioKCAyZMnk5OTw7Zt25rtW7NmDZdccgmXX345S5cuDVMJRUREIlPUJRabNm1i\n9+7dFBYWkp+fT35+vm9fY2MjDz30EM8++yzLli1j3bp1fP/992EsrYiISGSJusRi/fr1ZGVlAZCa\nmkpFRQUulwuA8vJyevXqhdVqJSYmhjPPPJP3338/nMUVERGJKFGXWDidTlJSUnzbVqsVh8Ph+/ng\nwYN8/fXX1NfXs3HjRpxOZ7iKKiIiEnFiw12AcPN6vb6fTSYT8+bNIzc3F4vFwpAhQ9p0jpSUJGJj\nzZ1VRABsNkunnj/SKB7+FBN/iok/xcSfYtJcR+MRdYmF3W5vVgtRUlKCzWbzbZ9xxhm8+OKLADz2\n2GMMHjz4qOcsL682vqCHsdksOBxVnXqNSKJ4+FNM/Ckm/hQTf4pJc22NR2vJR9Q1hWRmZrJq1SoA\niouLsdvtJCcn+/Zfd911lJaWUl1dzbp16xg7dmy4iioiIhJxoq7GIiMjg7S0NHJycjCZTOTl5VFU\nVITFYiE7O5vLLruMadOmYTKZmDFjBlarNdxFFhERiRgm7+GdDCQonV2Npqq65hQPf4qJP8XEn2Li\nTzFpTk0hIiIi0qUosRARERHDKLEQERERwyixEBEREcMosRARERHDKLEQERERwyixEBER6Wbc9R5K\nyqtx13tCfu2omyBLREQk1Nz1HipcbnonJ5AQ1/a1pdp7nKexkcK1O9myw0FZpRtrrwTGjLAxefxw\nzDGhqUtQYiEiIlHJXe9hn/MgnnpPm1/2oXrRB3tc4dqdrPlwr2+7tNLt274ia0Sb7rGjlFiIiEiX\nEJZv9VVurJajv7RD/aIP5jh3vYctOxwB923Z4eSScantimuwlFiIiEhYRcK3+lC+6IM9rsLlpqzS\nHfC48qpaKlxu7ClJAfcbSZ03RUTEUO3tOHjopV1a6cbLDy/twrU7DT/uaC/tQGUO5hho24veyON6\nJydg7ZUQcF+KJZHeyYH3GU2JhYiItKg9SYKnsZEX1+zg3mc3cPfCDdz77AZeXLMDT2Njq+cP5qUd\nypd9qF/0wR6XEGdmzAhbwH1jRvQLSTMIqClERCQsgu1P0JHrtaejYjDNDME0FwRbfR/scYde2qUB\njm3ppR3MMfDDi/7wmBzS2os+2OMAJo8fDjQlV+VVtaRYEhkzop/v81BQYiEiEkIdHQ4Yio6K0P4k\nIdh+AcG+tEP5sg/Hiz7Y48wxMVyRNYJLxqWGNHE9nBILEZEOaO+LPthRAqHsqBhMkhBsDUKkfKsP\n9Yu+owlCQpw5JB01A1FiISLyX+1JEoJ50XdkOGAoRyUEkyQEW4MA4f1Wb46Pw1NXf9R/73C96MOZ\nIARLiYWIRL2u3p8g1MMPg0kSOlKDEM5v9bZ+PXE4qtp1TKS96ENNo0JEJOq1d9hisCMSgu3tH+pR\nCcGOLpg8fjhZpw2hb69EYkzQt1ciWacNaXPHwUMv7fb2CQj2OOkcqrEQkS6rIzMxtnUERCT0Jwj1\nqAQIrpmhK3QclPBTYiEiXY4hMzG2cQREJPQnCMeohI4kCWouiG5RmVgUFBSwdetWTCYTubm5pKen\n+/YtW7aMV155hZiYGE466STuueeeMJZUpHsI1ciJYI6LlP4EoeyoeDglCdJeUZdYbNq0id27d1NY\nWMiuXbvIzc2lsLAQAJfLxeLFi3njjTeIjY1l2rRpfPzxx5xyyilhLrVI19FVR04Ee1ywSUJHJyJq\n7ws7HB0VRYIRdYnF+vXrycrKAiA1NZWKigpcLhfJycnExcURFxdHdXU1SUlJ1NTU0Lt37zCXWKRr\n6OojJzqyAFMk9SdQDYJ0dVGXWDidTtLS0nzbVqsVh8NBcnIyCQkJzJw5k6ysLBISEpg4cSLDhg0L\nY2lFOk9nN09EykyMoP4EIkaKusTiSF6v1/ezy+Vi4cKFvP766yQnJ3P11Vfz+eefM3LkyFbPkZKS\nRGxs535TsdksnXr+SKN4+GtrTDyeRp5/tZgN2/fhOFCDrU8PzjxpINMuSsNsDlzzUFvXwLZdpQH3\nbdtVyg2X9CAxvvmfk33Og5RVtVyDYI6Pw9avZ8D9mT8azCvvfBXg80EMGdSnxXsL9rjDDWnTb0Uu\n/d/x1x1j4m6oo7y2gpTE3iTExrfr2I7GI+oSC7vdjtPp9G2XlJRgszWN1961axdDhw7FarUCcNpp\np7F9+/ajJhbl5dWdV2Ca/pHVLvoDxaM5d72nXZ3yXlyzo1nNQ0l5Da+88xXVNXUtNk+UlFfjKK8J\nuM95oIZdX5f6fWv31HuwWlquQfDU1bf473jR2GOorqnza5q4aOwxrf7bB3tctAjm/06dp44KdxW9\nEyzEm9v3gurq6jx1mJMb8bhius29eRo9FO1cyTZHMeXuA6Qk9CHdlsak4RMxxxz970Nbn5HWko+o\nSywyMzN5+umnycnJobi4GLvdTnJyMgCDBw9m165d1NbWkpiYyPbt2xk3blyYSywSWDBDK0PZPBHu\nmRiDGQERSl39hd2RF1So76291+vO91a0cyVv7X3Xt13mLvdt/2rEzzutnIeLusQiIyODtLQ0cnJy\nMJlM5OXlUVRUhMViITs7m+nTpzN16lTMZjNjxozhtNNOC3eRJUqEYkhmqCd2CvXIiUNMMR5MifWY\nGmKArpVYdPQbZahebMG8oEJ9b8Fer7veW52njm2O4oD7PnEWc3HqhJAkQ1GXWADMnj272fbhTR05\nOTnk5OSEukgSxUI5JDPUEzuFeuREJLy0g/1GacS9fe9y4PEcvdo/2BdUqO8tmOt153urcFdR7j4Q\ncF9Z7QEq3FXYkvq2eE2jRGViIdKVRELNA0TGyIlwvbTbqiPfKEN5b8G8oEJ9b8FerzvfW+8ECykJ\nfShzl/vtsyb2oXdCaDqpahEykU7grvdQUl7d4mJUh/9eKBezgh8WirL2jiUmsRpr79h2LRRlivFg\nSqjBFNP6vR2uzlOHo7qUOk9dm49pr6P9MW7t2of++Je5y/Hi9f3xL9q5sk3Xbeu9teWl1tI1Qnlv\nh15QgbT0ggr1vQV7ve58b/HmeNJtaQH3ndwvLWR9eVRjIWKg9jZrhKPmAbzEHfM5iT2KSXCXk5iQ\nQpwtDUg9yr21/5tvKJsmgq0GDvbbYTD3Fuw3ylDf26EX1OHftA9p6QUV6nsL9nrd+d4AJg2fCDT9\n+5bVHsCa2IeT+6X5Pg8FJRYiBmpvs0ao+zyAf9VseRur1IOp0g1l9X2o//gHc2/BvNQg9PcG7X9B\nhfregr3ekfdWXnuAlG50b+YYM78a8XMuTp0QtlFHSixEWtGekRrNmjViPJji3HjrE6DR3GKHSqP6\nPLR1aGWw32CDOS7U7dKh/OPfkXsL5htlqF9sENwLKpT3Fuz1jry3ts5jESn3dvi1Q9FRMxAlFhIV\n2j/Uq/0jNZqaNWqIHfoF5pT9mBJq8boT8ZT3p3zviS02a3R0SGZ7hlYG+w02mONCXX0P7f8mCsH9\n8e9ITUCw3yhD/WI7/BxtfUGF8t46cr1D4s3x2JItOGqOPiFUpN1bOCmxkG4t2Db+YJffTk7dSUPf\n3b7PTIm1xAzcTWy8md7J5wY8LtjRFqFsLgjmuHBU3wfzTRTa/8ffiN737f1GacSLra3JVkeF6t6C\nvV5HdOd7M4oSC+nWgqlSD3aOCFOMB7O1hIYAx5mtJf8dRdFywtDeIZmhbC4I5rhwVN8ffu22fhOF\n9v/xN6ImIFgdebF19emrI/El2lbd+d6OpOGm0m21VqW+zdHKUK/DR2rEeDAlVMN/h1YeGqkR8Dh3\nFXWmg4HLYjrY4hCxYHRk+OGk4RM5Z8hZ9E1MwYSJvokpnDPkrKN+gw3muGCOCeeQuUN//NtaExBM\nHMMl3hzPgGRbl00qpPtQjYV0WxXuKspqy8Hkv6+strzloV7JCaT0iqOy9za/vhK9Kn7U8rLdHfym\nHYqhlRB81Wwwx4W6XTqUIrkNXKQzKbGQbisxJgnqe0B8gFU563s07Q8gIc5MnxFfURPj31eiT/9k\nEuLOCnhcsNXjoewrcWR5g6maDea4ULdLh1I0VXGLtIWaQqTbqqnxUl9qD7ivocxOTY034L46Tx3u\nHt8F3Ofu8Z3hzQzBzIzYVWbY62ztaZoQka5BNRYSUdz1HvY5D+Kp9xx15ETv5AR6VaRTCZhTSjDF\n1+Ct64Gn3N5qk0ZrzQzlbmObGUI9tFJEpLMpsZCI0GxeiSo3VsvR55VIiDOTMaI/az4cRcO3I5pN\nWJVxmr3FxCSUzQzhGFopItKZ1BQiYdOexZsOzStR6qqG+GpKXdWs+XAvhWt3tnrcoQW3+ib3xFSX\nRN/knkddcCuUzQzBLIh0JPX2F5GuRDUWEnLt7azorvewecd+Yod+5jdKY/OO+BbnlYDgJ58K1aiE\ncM6HICLSGZRYSMi1d2KnCpebyt7biBvoP0qjygQVroyjTizV3smnQjkqIRKGVoqItJUSC+mw9sy/\nEExnxR49TMT1LQl4TKy1hB49AkxUYZBQDCWMpKGVIiJHo8RCghbM/AvBdFasbayGuABzUQDE1VDb\nWI2FHh26l65A8yGISHegzpsStGDmX+idYCHe2zPgvnhvzxYXs7ImpgQ8xpqY0qYOjiIiEhpKLMSn\nPaM0gl2rwttoxlMWeNIqT5kdb6N/TUdrozTSbergKCLSlURlU0hBQQFbt27FZDKRm5tLeno6APv3\n72f27Nm+39uzZw+33347F110UbiKGhKhatKApo6Yrl3DMdd5/Catcu8dToXLHbCTpSaDEhGJDFGX\nWGzatIndu3dTWFjIrl27yM3NpbCwEID+/fuzZMkSABoaGpgyZQrjx48PZ3FDIpjlt4OdRKp3cgLW\nXj0o3eM/aVXfXoktzoapyaBERCJD1DWFrF+/nqysLABSU1OpqKjA5XL5/d4//vEPLrjgAnr2DNwf\noLsItkkj3hzPyf1GB9x3Ut/RLb70E+LMjBlha9poNON1J8F/mz/GjOh31DkmNBmUiEjXFnWJhdPp\nJCXlh46AVqsVh8Ph93t/+9vfuPTSS0NZtLBoS5NGS+r3nEj9vmNprO2BtxEaa3tQv+9Y6vec2Oo1\nfbNh9kokxgR9eyUedTZMERGJDFHXFHIkr9d/hcstW7Zw/PHHk5yc3KZzpKQkERt79NkcO8Jm65yR\nD70aEuiXZMVRXep/zSQrqYMHkRDrXztQW9fAJ7vKaSj3b9L4xFWO5ZIeJMa3/Hjdcvmp1NY1UF7p\nJqVXQqu/G0hnxSOSKSb+FBN/iok/xaS5jsYj6hILu92O0+n0bZeUlGCz2Zr9zltvvcXYsWPbfM7y\n8mrDyheIzWbB4Wi55qCj0qyjeKvaf0rp0dZRVJa7AbffvpLyahzl/51b4lCTxn85D9Sw6+vSNs10\nGQtUVdTQnrvr7HhEIsXEn2LiTzHxp5g019Z4tJZ8RF1TSGZmJqtWrQKguLgYu93uVzPxySefMHLk\nyHAULywmDZ/IOUPOom9iCiZM9E1M4ZwhZ7U64qKpE2bgjpYplpY7YYqISPcWdTUWGRkZpKWlkZOT\ng8lkIi8vj6KiIiwWC9nZ2QA4HA769o2eGRCDmVL6UCfMNR/u9dvXlk6YIiLSPUVdYgE0m6sC8Kud\nePXVV0NZnC7D22jG6+6BN84MbcgLDnW23LLDSXlVLSmWRMaM6KdOmCIiUSwqE4vurj2LggF4Ghsp\nXLuTLTsclFW6sfZKYMwIG5PHD8cc03JrWbBLkouISPelxKIbCWYGTYDCtTubNWmUVrp921dkjTjq\nddu7JLmIiHRfUdd5szsLZlEwd72HLTv85/GApiYOd72ns4orIiLdkBKLbiLYGTQrXG7KKv2HkwKU\nV9VS4Qq8T0REJBAlFt1EsDNoatioiIgYSYlFN3FoUbBAWlsUrNnaHUfQsFEREWkvJRbdRLw5nnRb\nWsB9J/dLa3V0iNbuEBERo2hUSDdyaKbMT5zFlNUewJrYh5P7pbU6gyZo2KiIiBhHiUU3EswMmofT\nsFEREekoJRbdULw5HltS9ExJLiIiXYf6WIiIiIhhlFiIiIiIYZRYdHF1njq+dzlanOAqEHe9h5Ly\nas2aKSIiIac+Fl1UMOt+BLuYmIiIiFGUWHRRh9b9OOTQuh8Avxrx84DHdHQxMRERkY7S19guKJh1\nP7SYmIiIdAVKLLqgYNb90GJiIiLSFSix6IKCWfdDi4mJiEhXoMSiCwpm3Q8tJiYiIl2BOm92UYev\n+1Fee4CUNqz7cWjRsC07nJRX1ZJiSWTMiH5aTExEREJGiUUXdfi6H+bkRjyumKOu+6HFxEREJNyi\nMrEoKChg69atmEwmcnNzSU9P9+3bt28ft912G/X19YwePZoHH3wwjCX977ofyRYcNf4dNluixcRE\nRCRcoq6PxaZNm9i9ezeFhYXk5+eTn5/fbP+8efOYNm0aK1aswGw2891334WppCIiIpEn6hKL9evX\nk5WVBUBqaioVFRW4XC4AGhsb+eijjxg/fjwAeXl5DBo0KGxlFRERiTRR1xTidDpJS/thxIXVasXh\ncJCcnExZWRk9e/Zk7ty5FBcXc9ppp3H77bcf9ZwpKUnExnZuXwabzX+IaTRTPPwpJv4UE3+KiT/F\npLmOxiPqEosjeb3eZj/v37+fqVOnMnjwYGbMmMFbb73FOeec0+o5ysurO7WMNpsFh6PtfSy6O8XD\nn2LiTzHxp5j4U0yaa2s8Wks+oq4pxG6343Q6fdslJSXYbE3zP6SkpDBo0CCOOeYYzGYzY8eO5csv\nvwxXUUVERCJO1CUWmZmZrFq1CoDi4mLsdjvJyckAxMbGMnToUL7++mvf/mHDhoWrqCIiIhEn6ppC\nMjIySEtLIycnB5PJRF5eHkVFRVgsFrKzs8nNzWXOnDl4vV5GjBjh68gpIiIiRxd1iQXA7Nmzm22P\nHDnS9/Oxxx7L8uXLQ10kERGRbiHqmkIijbvewz7nQS17LiIiESEqaywigaexkcK1O9myw0FZlRur\nJYExI2xMHj8cc4zyQRER6ZqUWHRRhWt3subDvb7t0kq3b/uKrBHhKpaIiEir9NW3C3LXe9iywxFw\n35YdTjWLiIhIl6XEoguqcLkpq3QH3FdeVUuFK/A+ERGRcFNi0QX1Tk7A2ish4L4USyK9kwPvExER\nCTclFl1QQpyZMSNsAfeNGdGPhLjOXZdEREQkWOq82UVNHj8caOpTUV5VS4olkTEj+vk+FxER6YqU\nWHRR5pgYrsgawSXjUjHHx+Gpq1dNhYiIdHlqCuniEuLMDOzXU0mFiIhEBCUWIiIiYhglFiIiImIY\n9bEQEZGI8/TT/8sXX3xGWVkptbW1DBo0mF69elNQsOCox/7rX6/Ss2cy48adG3D/k08+xq9+lcOg\nQYM7VMbbbptFQkICc+c+1qHzRBolFiIiEhLueg8VLje9kxM63G/sN7+5FWhKEr76ahezZv22zcde\neOFFre6/5ZbbO1Q2gPLyMr7++j/U1blxuVwkJyd3+JyRQomFiIh0qmaLKla6sfbqvEUVN2/+kJde\nWkp1dTWzZt3Kli0f8dZbb9LY2MjYsZlMmzaDxYsX0qdPH4YNS2Xlyn9QV+dh9+7/cM455zFt2gxm\nzZrBbbfdybp1b3LwoItvvtnNt9/u5eabb2fs2EyWLn2BNWveYNCgwTQ0NJCTcyUZGac1K8ebb75B\nZubZuFxVvP32WiZO/DkAy5b9hbfeehOTKYYbb5xFRsZpfp8NHDiIe++9i8WLlwAwffoUHn54Ps8/\nv4jY2DgqKw+Qm5vH7353LzU1NdTW1nLrrXcwevRJfPDBBhYu/AMxMTFkZZ3P0KHHsmbN69x330MA\nzJ//MJmZP+Gss8YZGvfDtelftKSkhNGjR7No0aJOK4iIiHRPhxZVLK104+WHRRUL1+7slOvt2rWT\nxx9/hpEjRwHwhz88x6JFL/Daa//HwYOuZr+7bds27rnnAf70pz/z978X+p2rpGQ/jz76FLfcMptX\nXimisrKCoqK/sXDh88yePYePP94csAyrV68iK+t8srIu4M033wBgz55veOutN1m48AXuv/8h3njj\ntYCftaZXr17k5y+gtLSUn/3sFzz99EJuvHEWy5b9Ba/Xy2OPzWfBgif54x8X8+GHmzjllAyKi4tx\nu900NjbyySdb+fGP/yeYsLZZm2os/vnPf5KamkpRUREzZszo1AKJiEj3cbRFFS8Zl2r4cPrhw08g\nPj4egMTERGbNmoHZbObAgQNUVlY2+93Ro0eTmJjY4rnS008BwG6343K52Lt3D8cfn0pCQiIJCYmM\nGpXmd8x3332Lw1FCevopeDwe5s9/mPLycnbs+ILRo08iJiaGIUOGMmfOfbz55mq/z/bt+67F8owe\n3XQ9q7Uvf/nLcyxfvoT6+noSExM5cKCc+Ph4UlJSAHjkkScAyMw8iw0b3qNv336kp59CXFxcO6LZ\nfm2qsfj73/9Obm4uNTU1bN7clJ1t3bqVyZMnc9VVVzFz5kxcLheNjY08+OCDXHbZZVx22WW89lpT\n5jV+/Hh2794NwMaNG7n88ssBmDJlCvn5+Vx11VV4PB5efPFF3zmnT5/uewCOvFZVVRXjx49nz549\nvjJeeOGF7NzZOdmviIgEJxyLKh56cX7//T4KC5fx2GNP88wzixgwYIDf78bGtv792mz+Ienxer14\nvRBzWPONyeR/zOrVr1NXV8e1117JdddNxeNpYN26NZjNMTQ2eo84v/9npiNO2tDQcFh5m+7t5Zdf\npF8/O3/842Jmz54DNJXryHMBTJgwkbVr1/Duu/8mO3tCq/drhKMmFh988AENDQ2ceeaZ/OIXv6Co\nqAiAO+64g4ceeoilS5dy+umn8/bbb/PKK6/gdDp5+eWXee655/jHP/6Bx9P6Et9JSUksXboUs9mM\n2+1m8eLFLF26lMGDB/PKK68EvNa///1vJk2axD//+U8AvvjiC3r16sXw4ZruWkSkKwnnoooHDhwg\nJSWFpKQkvvjic77//nvq6+s7dM6BAwfy1Ve7aGhooLy8nM8//8zvd9asWcWTT/6RF154kRdeeJH8\n/AWsWbOKE08cxSefbKWhoYGyslLuvnt2wM+SknpSXl6G1+ultNTJd9/t9btGRcUBBg8eAsDbb6+j\noaGB3r370NjoweEowev1cuedv6WqqooTTjgRp9PBZ58Vc8opGR26/7Y4alPIihUr+OUvf4nJZGLS\npElMmjSJm266icrKSkaMGAHANddcA8CDDz7Ij3/8Y6CpHagtfTIyMn64yT59+jBjxgxiYmL49ttv\nsdlslJWVBbzW/v37mTp1KrNmzeK1117jkksuadeNi4hI5zu0qOKaD/1fjp29qOIJJ4ygR48kfv3r\naZx88ilcfPEkHntsPunpPwr6nFZrX7KzJ3D99VM59thhjB6d1qxW48svdxAfn0Bq6g9fdH/0ozGU\nlZURExPDBRdcyKxZM/B6vdxww0wGDhzk91mvXr047bQzuO66qQwffgInnHCiXzkmTJjIww/nsW7d\nGi655DLWrHmDlStf4fbb53DvvXcBMH58FhaLBYDTT/8x1dXVfrUhnaHVxMLlcvHGG28wcOBAVq9e\nDUBjYyMbN27E6/WvbjGZTDQ2NrZ6wSOzxR+qrL5n/vz5rFy5kr59+zJ//nzfOQNdq3///qSmpvLR\nRx/x73//myVLlrR6XRERCY/OXFTxyKGjGRmn+UZomM1mHn/8mVaPv+CCc3E4qgBYufJNAJ55pulL\n8fHH/1C+448f7vt86NBjmDatqd/G1Kk5DBw4yPd7J5wwwjea4xCTycRLLzXV9l9++VVcfvlVzfYH\n+iw3N8+vrPfc84Dv51Gj0li2bIVv+/BRHgsX/rnZcV6vly1bNnPHHXcHCoHhWk0s/u///o/TTz+9\nWc3Dq6++yt/+9jf69OnDtm3bSE9P5/nnnychIYExY8bwyiuvMGXKFFwuF9deey3Lli0jOTmZffv2\nceyxx7Jhw4aA1yotLSUlJYW+ffty4MAB3n33Xc455xxSUlICXuvKK69k8uTJPPbYY4waNYqePXu2\n+aYLCgrYunUrJpOJ3Nxc0tNFlT3iAAAaUUlEQVTTffvGjx/PgAEDfBnoo48+Sv/+/dt8bhERae7w\nRRWNmscinEpLS5kx42ri4uI5//wJ2O1d9x2xb9933HPPnYwfn8WQIUNDcs1WE4sVK1Ywc+bMZp9d\ncMEFzJs3jz/+8Y8UFBQQGxuLxWJhwYIF9OjRg82bN5OTk4PH4+Haa68lPj6eadOmcc8993Dcccc1\na/o43KhRozj22GO59NJLOeaYY7j55pt54IEHGDduHAsWLPC7FsBPfvITcnNzueuuu9p8w5s2bWL3\n7t0UFhaya9cucnNzKSxsPsTo2WefbVeiIiIiR5cQZ8aekhTuYnTYlCnXMGXKNeEuRpsMHDiI559f\nGtJrmryB2hkixLZt25g7dy7Lly9v8zFPPvkkgwYN4le/+hUAEyZMYMWKFb5Z0caPH8+rr77arsTi\nUDVaZ7HZLJ1+jUiiePhTTPwpJv4UE3+KSXNtjYfNZmlxX8TOvPnggw+ydetWX+1FWzmdTtLSfhh3\nbLVacTgczaZbzcvL49tvv+XUU0/l9ttvP2pnl5SUJGJjO7dar7V/xGikePhTTPwpJv4UE3+KSXMd\njUfEJhb333+/Iec5ssLm5ptv5ic/+Qm9e/dm5syZrFq1igkTWh/3W15ebUhZWqKMujnFw59i4k8x\n8aeY+FNMmjOixiLqlk232+04nU7fdklJCTabzbf9i1/8gr59+xIbG8vZZ5/Njh07wlFMERGRiBR1\niUVmZiarVq0CoLi4GLvd7msGqaqqYvr06dTV1QFNk4OdcMIJYSuriIgEdsMN1/pNTvWnPz3D8uWB\nOypu3vwh9957JwBz5tzmt//vfy9k8eKFLV5v584v+eabphmk8/Luxu2uDbboPldccQlPPtn9llSP\nusQiIyODtLQ0cnJyePjhh8nLy6OoqIjVq1djsVg4++yzmTx5Mjk5OVit1qM2g4iISNvUeepwVJdS\n56nr8Lmysy9g7drVzT576621ZGWdf9Rj5817vN3Xe/vttezZ8w0Av/vdXBISWl5fpC0+//wzvF6v\nb+XV7iRi+1h0xOzZs5ttjxw50vfz1VdfzdVXXx3qIomIdFueRg9FO1eyzVFMufsAKQl9SLelMWn4\nRMwxwXV8P++88/n1r6dz0003A00vapvNhs1m54MPNvLcc38iLi4Oi8XCgw/Oa3bsxInnsXLlm3z4\n4Sb+8Icn6NWrD3379vMtg56f/wAORwk1NTVMmzaDAQMG8v/+XxFvv72WlJQU7r//bv7610Jcrirm\nzn2Q+vp6YmJimDPnPkwmE/n5DzBo0GB27vySESNOZM6c+/zKv3r161x00S945523+Pjjzb5JvZ54\n4lE+/XQ7ZrOZO+64m+OPH+732YEDBygqepmHH36k2f3MmjWD449PBeCqq67hoYea+iI2NDRw772/\nY/DgIbz++kpWrCjEZDKRk3MllZWVOJ0Orr/+1wBce+21zJjxG4YPD762PupqLEREJLSKdq7krb3v\nUuYux4uXMnc5b+19l6KdK4M+Z0qKlUGDBvPpp9sBWLt2tW+BraqqKvLyHuaZZxaRlNSTjRvXBzzH\nwoXPsGDBAp544g9UVBz477GVnHHGmTzzzCIefHAuixcvJDV1OD/+8VhuuGEWo0ef5Dv+uef+xM9+\ndjHPPLOIX/7yUp5/vmkyyS+++IwbbpjJc8/9lfXr36OqqnlnyMbGRtatW8P48U3Lqq9Z09Q8/8EH\nGykp2c+iRS9www0zefPN1QE/a83xx6dy2213UVrq5Nprr+fppxcyceLPKSr6G9XVB3nhhef4/e8X\n8fjjz7B69eucd14277zzFtA02/aBAwc6lFSAEgsREelEdZ46tjmKA+77xFncoWaR7OwJvhfte+/9\nm3POOQ9oWndq/vyHmTVrBlu2fERlZUXA4/ft2+ersT60OJfF0ovPPivm17+eRn7+Ay0eC00JxJgx\npwJNU4l/+eUXAAwePJS+ffsRExNDv342Dh50NTvu448307//AAYMGMD48dm8++6/aWhoYMeOzzn5\n5B/5ynP99b8O+FlrRo1qSnys1r787W8vMXPm9bz88otUVlbw9df/4ZhjjiMhIRGLxcK8eY/Tq1dv\nhgw5hi+++Jz16981pPk/KptCREQkNCrcVZS7DwTcV1Z7gAp3FbakvkGde9y4c/nrX58nO/sChg49\nhl69egEwd+5DLFjwBMcdN4zHH5/f4vGHL39+aOqB1atfp7Kykt///jkqKyu57roprZTgh7Ws6usb\nMJmaznf4omSHn/uQ1atf5/vv93HNNVcAUFtbywcfbCAmxozX27y/RaDPWltWPS6u6bW+ePFCfvzj\nM/nFLy5l3bo1vP/+uwHPBU0Lmq1bt4bvv9/H3Xff2cr9to1qLEREpNP0TrCQktAn4D5rYh96JwQ/\nGVNSUk9SU0/gr3/9s68ZBODgQRf9+w+gqqqKzZs/anGp9H79bHz11Vf/XaTrI6BpqfWBAwcRExPD\n22+v9R1rMpnweDzNjh81ajSbN38IwMcff8TIkaOOWub6+nree+8d35LqL7zwIrfeegdr1qxqdr4d\nOz7nscfmB/ysZ8+elJY2TZuwc+eXVFf7z6V04EDTsuper5d3332b+vp6jj32OL75ZjfV1dW43W5+\n+9ub8Hq9jB2bydatm3G5qhgyZMhR7+FoVGMhIiKdJt4cT7otjbf2vuu37+R+acSb4zt0/uzsCTz8\ncB55eQ/5Pps06Vf8+tfTGTr0GK68cirPP7+IGTNu8jt2xoybuOWWW+jXz+5bSOycc8YzZ85tfPrp\ndiZO/Dl2u50///lZfvSjMTzxxAKSkn5Y6+S6625k7tyHePXVfxIbG8fdd9/XrPYgkA0b3iM9/Uf0\n7v1DsnXuuVksWvQH7rzzXo49dhg33XQdALffPofU1OG8887bzT4bNux4EhN7cOON0zj55B8xYMAg\nv+tcfPEk/vd/FzBgwCAuvXQyjzySzyefbGX69Bv57W+bYjF58hWYTCbi4uI49thhnHji0ROjtojo\ntUK6Cq0VElqKhz/FxJ9i4i9cMTk0KuQTZzFltQewJvbh5H4dGxViFD0n4Ha7mTnzep544g8MGzYw\netcKERGRyGCOMfOrET/n4tQJVLir6J1g6XBNhRhj+/ZPWLCggCuumNJszayOUGIhIiIhEW+OD7qj\npnSOk046mb/8pe0rhLeFOm+KiIiIYZRYiIiIiGGUWIiIiIhhlFiIiIiIYZRYiIiIiGGUWIiIiIhh\nlFiIiIiIYZRYiIiIiGGUWIiIiIhhlFiIiIiIYZRYiIiIiGGUWIiIiIhhlFiIiIiIYaIysSgoKGDy\n5Mnk5OSwbdu2gL/z2GOPMWXKlBCXTEREJLJFXWKxadMmdu/eTWFhIfn5+eTn5/v9zs6dO/nggw/C\nUDoREZHIFnWJxfr168nKygIgNTWViooKXC5Xs9+ZN28et956aziKJyIiEtFiw12AUHM6naSlpfm2\nrVYrDoeD5ORkAIqKijjjjDMYPHhwm8+ZkpJEbKzZ8LIezmazdOr5I43i4U8x8aeY+FNM/CkmzXU0\nHlGXWBzJ6/X6fj5w4ABFRUX8+c9/Zv/+/W0+R3l5dWcUzcdms+BwVHXqNSKJ4uFPMfGnmPhTTPwp\nJs21NR6tJR9R1xRit9txOp2+7ZKSEmw2GwAbNmygrKyMK6+8klmzZlFcXExBQUG4iioiIhJxoi6x\nyMzMZNWqVQAUFxdjt9t9zSATJkzgX//6Fy+//DLPPPMMaWlp5ObmhrO4IiIiESXqmkIyMjJIS0sj\nJycHk8lEXl4eRUVFWCwWsrOzw108ERGRiBZ1iQXA7Nmzm22PHDnS73eGDBnCkiVLQlUkERGRbiHq\nmkJERESk8yixEBEREcMosRARERHDKLEQERERwyixEBEREcMosRARERHDKLEQERERwyixEBEREcMo\nsRARERHDKLEQERERwyixEBEREcMosRARERHDKLEQERERwyixEBEREcMosRARERHDKLEQERERwyix\nEBEREcMosRARERHDKLEQERERwyixEBEREcPEhrsA4VBQUMDWrVsxmUzk5uaSnp7u2/fyyy+zYsUK\nYmJiGDlyJHl5eZhMpjCWVkREJHJEXY3Fpk2b2L17N4WFheTn55Ofn+/bV1NTw8qVK1m2bBkvvfQS\nX331FVu2bAljaUVERCJL1CUW69evJysrC4DU1FQqKipwuVwA9OjRg7/85S/ExcVRU1ODy+XCZrOF\ns7giIiIRJeoSC6fTSUpKim/barXicDia/c6iRYvIzs5mwoQJDB06NNRFFBERiVhR2cficF6v1++z\nGTNmMHXqVK6//npOPfVUTj311FbPkZKSRGysubOKCIDNZunU80caxcOfYuJPMfGnmPhTTJrraDyi\nLrGw2+04nU7fdklJia+548CBA3z55ZecfvrpJCYmcvbZZ7N58+ajJhbl5dWdWmabzYLDUdWp14gk\nioc/xcSfYuJPMfGnmDTX1ni0lnxEXVNIZmYmq1atAqC4uBi73U5ycjIADQ0NzJkzh4MHDwLwySef\nMGzYsLCVVUREJNJEXY1FRkYGaWlp5OTkYDKZyMvLo6ioCIvFQnZ2NjNnzmTq1KnExsZy4oknct55\n54W7yCIiIhHD5A3UyUDapbOr0VRV15zi4U8x8aeY+FNM/CkmzakpRERERLoUJRYiIiJiGCUWIiIi\nYhglFiIiImIYJRYiIiJiGCUWIiIiYhglFiIiImIYJRYiIiJiGCUWIiIiYhglFiIiImIYJRYiIiJi\nGCUWIiIiYhglFiIiImIYJRYiIiJiGCUWIiIiYhglFiIiImIYJRYiIiJiGCUWIiIiYhglFiIiImIY\nJRYiIiJiGCUWIiIiYpjYcBcgHAoKCti6dSsmk4nc3FzS09N9+zZs2MDjjz9OTEwMw4YNIz8/n5gY\n5V8iIiJtEXVvzE2bNrF7924KCwvJz88nPz+/2f7777+fp556ipdeeomDBw/yzjvvhKmkIiIikSfq\nEov169eTlZUFQGpqKhUVFbhcLt/+oqIiBgwYAIDVaqW8vDws5RQREYlEUZdYOJ1OUlJSfNtWqxWH\nw+HbTk5OBqCkpIT33nuPcePGhbyMIiIikSoq+1gczuv1+n1WWlrKjTfeSF5eXrMkpCUpKUnExpo7\no3g+NpulU88faRQPf4qJP8XEn2LiTzFprqPxiLrEwm6343Q6fdslJSXYbDbftsvl4vrrr+e3v/0t\nZ511VpvOWV5ebXg5D2ezWXA4qjr1GpFE8fCnmPhTTPwpJv4Uk+baGo/Wko+oawrJzMxk1apVABQX\nF2O3233NHwDz5s3j6quv5uyzzw5XEUVERCJW1NVYZGRkkJaWRk5ODiaTiby8PIqKirBYLJx11ln8\n85//ZPfu3axYsQKAn/3sZ0yePDnMpRYREYkMUZdYAMyePbvZ9siRI30/b9++PdTFERER6TairilE\nREREOo8SCxERETGMEgsRERExjBILERERMYwSCxERETGMEgsRERExjBILERERMYwSCxERETGMEgsR\nERExjBILERERMYwSCxERETGMEgsRERExjBILERERMYwSCxERETGMEgsRERExjBILERERMYwSCxER\nETGMEgsRERExjBILERERMYwSCxERETGMEgsRERExTFQmFgUFBUyePJmcnBy2bdvWbJ/b7eauu+5i\n0qRJYSqdiIhI5Iq6xGLTpk3s3r2bwsJC8vPzyc/Pb7b/kUceYdSoUWEqnYiISGSLusRi/fr1ZGVl\nAZCamkpFRQUul8u3/9Zbb/XtFxERkfaJusTC6XSSkpLi27ZarTgcDt92cnJyOIolIiLSLcSGuwDh\n5vV6O3yOlJQkYmPNBpSmZTabpVPPH2kUD3+KiT/FxJ9i4k8xaa6j8Yi6xMJut+N0On3bJSUl2Gy2\nDp2zvLy6o8Vqlc1mweGo6tRrRBLFw59i4k8x8aeY+FNMmmtrPFpLPqKuKSQzM5NVq1YBUFxcjN1u\nV/OHiIiIQaKuxiIjI4O0tDRycnIwmUzk5eVRVFSExWIhOzubm2++me+//57//Oc/TJkyhcsuu4yL\nLroo3MUWERGJCFGXWADMnj272fbIkSN9Pz/11FOhLo6IiEi3EXVNISIiItJ5lFiIiIiIYZRYiIiI\niGGUWIiIiIhhlFiIiIiIYZRYiIiIiGGUWIiIiIhhlFiIiIiIYZRYiIiIiGGUWIiIiIhhlFiIiIiI\nYZRYiIiIiGGUWIiIiIhhlFiIiIiIYZRYiIiIiGGUWIiIiIhhlFiIiIiIYZRYiIiIiGGUWIiIiIhh\nlFiIiIiIYZRYiIiIiGGiMrEoKChg8uTJ5OTksG3btmb73n//fS699FImT57M73//+zCVUEREJDJF\nXWKxadMmdu/eTWFhIfn5+eTn5zfb//DDD/P000+zfPly3nvvPXbu3BmmkoqIiESeqEss1q9fT1ZW\nFgCpqalUVFTgcrkA2LNnD71792bgwIHExMQwbtw41q9fH87iioiIRJSoSyycTicpKSm+bavVisPh\nAMDhcGC1WgPuExERkaOLDXcBws3r9Xb4HDabxYCShP8akUTx8KeY+FNM/Ckm/hST5joaj6irsbDb\n7TidTt92SUkJNpst4L79+/djt9tDXkYREZFIFXWJRWZmJqtWrQKguLgYu91OcnIyAEOGDMHlcrF3\n714aGhpYt24dmZmZ4SyuiIhIRDF5jWgLiDCPPvooH374ISaTiby8PD799FMsFgvZ2dl88MEHPPro\nowCcf/75TJ8+PcylFRERiRxRmViIiIhI54i6phARERHpPEosRERExDBRP9y0KysoKGDr1q2YTCZy\nc3NJT08Pd5HCauPGjdxyyy2ccMIJAIwYMYL77rsvzKUKjx07dnDTTTdxzTXXcNVVV7Fv3z7uvPNO\nPB4PNpuNBQsWEB8fH+5ihtSRMZkzZw7FxcX06dMHgOnTp3POOeeEt5Ah9sgjj/DRRx/R0NDADTfc\nwMknnxzVz8mR8Vi7dm1UPyM1NTXMmTOH0tJS3G43N910EyNHjuzwM6LEoos6fOrxXbt2kZubS2Fh\nYbiLFXZnnHEGTz31VLiLEVbV1dU89NBDjB071vfZU089xRVXXMFPf/pTHn/8cVasWMEVV1wRxlKG\nVqCYANx2222ce+65YSpVeG3YsIEvv/ySwsJCysvL+eUvf8nYsWOj9jkJFI8zzzwzqp+RdevWcdJJ\nJ3H99dfz7bffMm3aNDIyMjr8jKgppItqbepxiW7x8fE8++yzzeZY2bhxI+eddx4A5557btRNRR8o\nJtHu9NNP58knnwSgV69e1NTURPVzEigeHo8nzKUKrwsvvJDrr78egH379tG/f39DnhElFl1Ua1OP\nR7OdO3dy4403cvnll/Pee++FuzhhERsbS2JiYrPPampqfNWVffv2jbpnJVBMAJYuXcrUqVO59dZb\nKSsrC0PJwsdsNpOUlATAihUrOPvss6P6OQkUD7PZHNXPyCE5OTnMnj2b3NxcQ54RNYVECI0KhuOO\nO45Zs2bx05/+lD179jB16lTeeOONqGojbgs9K00uvvhi+vTpw6hRo1i0aBHPPPMM999/f7iLFXJr\n1qxhxYoVPP/885x//vm+z6P1OTk8Htu3b9czArz00kt89tln3HHHHc2ei2CfEdVYdFGtTT0erfr3\n78+FF16IyWTimGOOoV+/fuzfvz/cxeoSkpKSqK2tBTQV/SFjx45l1KhRAIwfP54dO3aEuUSh9847\n7/CnP/2JZ599FovFEvXPyZHxiPZnZPv27ezbtw+AUaNG4fF46NmzZ4efESUWXVRrU49Hq1deeYXF\nixcDTSvRlpaW0r9//zCXqmv4n//5H9/z8sYbb/CTn/wkzCUKv9/85jfs2bMHaOqDcmg0UbSoqqri\nkUceYeHChb5RD9H8nASKR7Q/Ix9++CHPP/880NT8Xl1dbcgzopk3u7Ajpx4fOXJkuIsUVi6Xi9mz\nZ1NZWUl9fT2zZs1i3Lhx4S5WyG3fvp358+fz7bffEhsbS//+/Xn00UeZM2cObrebQYMGMXfuXOLi\n4sJd1JAJFJOrrrqKRYsW0aNHD5KSkpg7dy59+/YNd1FDprCwkKeffpphw4b5Pps3bx733ntvVD4n\ngeIxadIkli5dGrXPSG1tLffccw/79u2jtraWWbNmcdJJJ3HXXXd16BlRYiEiIiKGUVOIiIiIGEaJ\nhYiIiBhGiYWIiIgYRomFiIiIGEaJhYiIiBhGM2+KSFjt3buXCRMmMGbMmGafjxs3juuuu67D59+4\ncSNPPPEEy5cv7/C5ROTolFiISNhZrVaWLFkS7mKIiAGUWIhIlzV69GhuuukmNm7cyMGDB5k3bx4j\nRoxg69atzJs3j9jYWEwmE/fffz/Dhw/n66+/5r777qOxsZGEhATmzp0LQGNjI3l5eXz22WfEx8ez\ncOFCevbsGea7E+me1MdCRLosj8fDCSecwJIlS7j88st56qmnALjzzju5++67WbJkCddeey2/+93v\nAMjLy2P69OksW7aMSy65hNdeew2AXbt28Zvf/IaXX36Z2NhY3n333bDdk0h3pxoLEQm7srIypkyZ\n0uyzO+64A4CzzjoLgIyMDBYvXkxlZSWlpaWkp6cDcMYZZ3DbbbcBsG3bNs444wwAJk6cCDT1sTj+\n+OPp168fAAMGDKCysrLzb0okSimxEJGwa62PxeGrDphMJkwmU4v7oanZ40hms9mAUopIW6gpRES6\ntA0bNgDw0UcfceKJJ2KxWLDZbGzduhWA9evXc8oppwBNtRrvvPMOAP/61794/PHHw1NokSimGgsR\nCbtATSFDhgwB4NNPP2X58uVUVFQwf/58AObPn8+8efMwm83ExMTwwAMPAHDfffdx33338eKLLxIb\nG0tBQQHffPNNSO9FJNppdVMR6bJOPPFEiouLiY3VdyCRSKGmEBERETGMaixERETEMKqxEBEREcMo\nsRARERHDKLEQERERwyixEBEREcMosRARERHDKLEQERERw/x/qmhGN9KZBAAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1rb7EzOWzl8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I don't think the classifier would perform much better if I were to train longer! I mean, I think it might perform *slightly* better. But the marginal increase in accuracy decreases with each additional epoch. Accuracy is quite flat after fifteen or so epochs, so I don't think training for longer will make much of a difference."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UxdW8dV4uoCq",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4.5"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "A97WOgV8uweq",
        "tags": [
          "HST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "Suggest at least three different ways to improve the performance of the classifier defined in this Part. For each way, explain why that would help. This is an open-ended question, and answers may vary. Do _not_ implement your suggestions, and do _not_ refer to techniques we have not covered in class (such as batch normalization or other techniques you may have heard of).\n",
        "\n",
        "If you suggest more than three ways, we will grade you for the best ones. However, we _will_ deduct points for patently wrong statements in any of your suggestions."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "12XxnmJwnYzN",
        "tags": [
          "ST"
        ]
      },
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ]
    },
    {
      "metadata": {
        "id": "Oi7W0xi74EGc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Cross-validation. This would help because it mitigates overfitting. Without cross-validation, we may be finding relationships in the training data that just occur by chance with our particular training dataset. With cross-validation, we vary our choice of training dataset.\n",
        "\n",
        "2. Try different Adam parameters (learning rate, learning rate decay, etc.). The current parameters may have found a local minimum instead of the global minimum.\n",
        "\n",
        "3. Try different weights.\n",
        "\n",
        "4. Alter the dataset by transforming old observations. By resizing, rotating, and compressing instances, we allow for greater generalization. In this way, we may train a model to be invariate to size, rotation, and scale. This helps with classifying distorted images."
      ]
    }
  ]
}